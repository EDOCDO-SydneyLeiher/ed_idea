{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import PyPDF2\n",
    "import os\n",
    "import uuid\n",
    "import docx2txt\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "main_page = \"https://sites.ed.gov/idea/policy-guidance/\"\n",
    "response = requests.get(main_page)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_items = soup.find_all(\"div\", class_=\"idea-file-item\")\n",
    "\n",
    "def get_pdf_text(pdf_link):\n",
    "    response = requests.get(pdf_link)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Could not download PDF from {pdf_link}\")\n",
    "        return \"Could not download PDF\"\n",
    "    random_filename = str(uuid.uuid4())\n",
    "    with open(random_filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    with open(random_filename, \"rb\") as f:\n",
    "        try:\n",
    "            pdf = PyPDF2.PdfReader(f)\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text()\n",
    "        except:\n",
    "            text = \"Could not extract text from PDF\"\n",
    "    os.remove(random_filename)\n",
    "    return text\n",
    "\n",
    "def get_doc_text(doc_link):\n",
    "    response = requests.get(doc_link)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Could not download document from {doc_link}\")\n",
    "        return \"Could not download document\"\n",
    "    try:\n",
    "        random_filename = str(uuid.uuid4())\n",
    "        with open(random_filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        if doc_link.lower().endswith(\".doc\"):\n",
    "            os.rename(random_filename, random_filename + \".doc\")\n",
    "            process = Popen([\"antiword\", random_filename+\".doc\"], stdout=PIPE)\n",
    "            text, err = process.communicate()\n",
    "            text = text.decode(\"utf-8\", \"ignore\")\n",
    "            #get rid of everything before filter : Text\\n\\ufeff\\n\n",
    "            index = text.find(\"filter : Text\\n\\ufeff\")\n",
    "            if index != -1:\n",
    "                text = text[index+15:]\n",
    "        elif doc_link.lower().endswith(\".docx\"):\n",
    "            text = docx2txt.process(random_filename)\n",
    "        os.remove(random_filename+\".doc\")\n",
    "        return text\n",
    "    except:\n",
    "        print(f\"Could not extract text from document {doc_link}\")\n",
    "        return \"Could not extract text from document\"\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "def process_file_item(file_item):\n",
    "    title = file_item.find(\"h3\").text\n",
    "    link = file_item.find(\"h3\").find(\"a\")[\"href\"]\n",
    "    topic_area = file_item.find(\"div\", class_=\"topic-area-list\").text\n",
    "    topic_area = re.sub(r\"Topic Areas: \", \"\", topic_area)\n",
    "    description = file_item.find(\"div\", class_=\"description\").text\n",
    "    description = re.sub(r\"Read More\", \"\", description)\n",
    "    all_links = file_item.find_all(\"a\")\n",
    "    pdf_links_titles = {link[\"href\"]:link.text for link in all_links if link[\"href\"].lower().endswith(\".pdf\")}\n",
    "    doc_docx_links_titles = {link[\"href\"]:link.text for link in all_links if link[\"href\"].lower().endswith(\".doc\") or link[\"href\"].lower().endswith(\".docx\")}\n",
    "    if pdf_links_titles or doc_docx_links_titles:\n",
    "        pdfs = [(link, pdf_links_titles[link], get_pdf_text(link)) for link in pdf_links_titles]\n",
    "        doc_docxs = [(link, doc_docx_links_titles[link], get_doc_text(link)) for link in doc_docx_links_titles]\n",
    "        return {\"title\": title, \"link\": link, \"topic_area\": topic_area, \"description\": description, \"docs\": pdfs+doc_docxs}\n",
    "    else:\n",
    "        link_soup = BeautifulSoup(requests.get(link).text, \"html.parser\")\n",
    "        main = link_soup.find(\"main\", class_=\"site-main\")\n",
    "        title = main.find(\"h1\").text\n",
    "        #remove the title from the main content\n",
    "        main_content = main.find('div', class_=\"idea-file-item\")\n",
    "        main_content.find('div', class_=\"topic-area-list\").decompose()\n",
    "        main_text = main_content.text\n",
    "        return {\"title\": title, \"link\": link, \"topic_area\": topic_area, \"description\": description, \"docs\": [(link, title, main_text)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "with tqdm(total=len(file_items)) as pbar:\n",
    "    for file_item in file_items:\n",
    "        result = process_file_item(file_item)\n",
    "        data.append(result)\n",
    "        pbar.update(1)\n",
    "\n",
    "import json\n",
    "with open(\"data.json\", \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "#cleanup\n",
    "#for every entry in data, we want to look at the docs and hopefully get rid of duplicates and other messy stuff\n",
    "#we want to remove any documents that are less than 100 characters long\n",
    "#then, if we see any 2 docs whose titles are the same except one ends in MS WORD and one ends in PDF then we only want to keep the MS WORD one\n",
    "#then we look for documents with a fuzz score over 90 and check those out\n",
    "\n",
    "for entry in data:\n",
    "    docs = entry[\"docs\"]\n",
    "    new_docs = []\n",
    "    for i in range(len(docs)):\n",
    "        if len(docs[i][2]) < 100:\n",
    "            continue\n",
    "        if docs[i][1].lower().endswith(\"pdf\"):\n",
    "            if any(docs[j][1].lower().endswith(\"ms word\") and docs[j][1].lower().replace(\"ms word\", \"\").replace(\"word\",\"\").strip() == docs[i][1].lower().replace(\"pdf\", \"\").strip() for j in range(len(docs))):\n",
    "                continue\n",
    "        new_docs.append(docs[i])\n",
    "    entry[\"docs\"] = new_docs\n",
    "\n",
    "with open(\"data_cleaned.json\", \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_cleaned.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "from llama_index.core import Document\n",
    "def get_documents(entry):\n",
    "    #for each document in the entry, create a document object\n",
    "    #metadata fields:\n",
    "    #   parent_title (passed to embedder and LLM)\n",
    "    #   document_title (passed to embedder and LLM)\n",
    "    #   parent_link (not used)\n",
    "    #   document_link (not used)\n",
    "    #   topic_area (passed to embedder)\n",
    "    #   description (passed to embedder)\n",
    "    #text field: document text (we have this)\n",
    "    parent_title = entry['title']\n",
    "    parent_link = entry['link']\n",
    "    topic_area = entry['topic_area']\n",
    "    description = entry['description']\n",
    "    documents = []\n",
    "    for doc in entry['docs']:\n",
    "        document_title = doc[1]\n",
    "        document_text = doc[2]\n",
    "        document = Document(text=document_text, metadata={\"parent_title\": parent_title, \"document_title\": document_title, \"parent_link\": parent_link, \"document_link\": doc[0], \"topic_area\": topic_area, \"description\": description})\n",
    "        document.excluded_embed_metadata_keys = [\"parent_link\", \"document_link\"]\n",
    "        document.excluded_llm_metadata_keys = [\"parent_link\", \"document_link\", \"topic_area\", \"description\"]\n",
    "        documents.append(document)\n",
    "    return documents\n",
    "\n",
    "documents = sum([get_documents(entry) for entry in data], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save the documents to disk\n",
    "import pickle\n",
    "with open(\"documents.pkl\", \"wb\") as f:  \n",
    "    pickle.dump(documents, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
